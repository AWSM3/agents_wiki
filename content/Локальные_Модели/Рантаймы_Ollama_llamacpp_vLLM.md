# 8.2. Рантаймы: Ollama / llama.cpp / vLLM (выбор под нагрузку)

- **Last reviewed**: 2026-01-15
- **Уровень**: Advanced

## Зачем эта страница

Собрать практический выбор рантайма для локальных моделей: один разработчик vs сервис на команду.

## Варианты (коротко)

- **Ollama**:
  - удобен для локальной разработки
  - часто поддерживает OpenAI‑совместимый API
- **llama.cpp**:
  - минимум зависимостей, хорошо для CPU/consumer
  - широко используется для GGUF
- **vLLM**:
  - предпочтителен для multi‑user/нагрузки
  - полезен, если нужна server‑архитектура и контроль ресурсов

## Как выбирать (правило большого пальца)

- **локально на ноутбуке** → Ollama или llama.cpp
- **внутренний сервис на команду** → vLLM + контроль доступа + аудит

## Связанные страницы

- [8.5 Безопасность локального хоста](Безопасность_Локального_Хоста.md)
- [6.5 Advanced guardrails](../Безопасность_Соответствие/Продвинутые_Ограничения.md)

