# 8.3. Квантизация (GGUF, AWQ) и практические профили качества

- **Last reviewed**: 2026-01-15
- **Уровень**: Advanced

## Зачем эта страница

Квантизация — основной способ сделать локальные модели дешевле/быстрее. Но это компромисс качества.

## Ключевые понятия

- **GGUF**: GPT-Generated Unified Format, популярный формат сжатия, чем ниже квант (Q8-Q1) тем меньше размер модели, но качество ответов существенно снижено.
- **AWQ**: популярный 4‑битный подход, есть свои преимущества перед GGUF или стандартной точностью за счёт особенности алгоритма оптимизации

## Практические профили (пример)

- **CPU‑only**: “качество ок на простых задачах”, скорость приемлемая
- **8–12GB VRAM**: GGUF Q4 и меньше, варианты под dev‑кейсы, но зависит от модели (максимум 20-27B)
- **16–24GB VRAM**: более качественные кванты, но всё ещё dev-профиль
- **32GB+ VRAM**: уверенно запускаем Qwen 3 Coder 30B A3B GGUF Q4-Q6
- **80GB+ VRAM**: многие модели полностью помещаются в VRAM, например GPT OSS 120B, но **не уровень GLM 4.6/Kimi K2/Qwen 3 Coder 480B** и подобных 

## Связанные страницы

- [8.4 Каталог моделей](Каталог_Рекомендуемых_Моделей.md)

