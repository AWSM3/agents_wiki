# 8.2. Рантаймы: Ollama / llama.cpp / vLLM (выбор под нагрузку)

- **Owner**: Platform (TBD)
- **Статус**: Draft
- **Last reviewed**: 2026-01-15
- **Уровень**: Advanced
- **Для ролей**: Dev / Arch / SRE

## Зачем эта страница

Собрать практический выбор рантайма для локальных моделей: один разработчик vs сервис на команду.

## Варианты (коротко)

- **Ollama**:
  - удобен для локальной разработки
  - часто поддерживает OpenAI‑совместимый API
- **llama.cpp**:
  - минимум зависимостей, хорошо для CPU/consumer
  - широко используется для GGUF
- **vLLM**:
  - предпочтителен для multi‑user/нагрузки
  - полезен, если нужна server‑архитектура и контроль ресурсов

## Как выбирать (правило большого пальца)

- **локально на ноутбуке** → Ollama или llama.cpp
- **внутренний сервис на команду** → vLLM + контроль доступа + аудит

## Связанные страницы

- [8.5 Безопасность локального хоста](08_05_Local_Host_Security.md)
- [6.5 Advanced guardrails](../06_Security_Compliance/06_05_Advanced_Guardrails.md)

